<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Full Report</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="
        fivegoldstars.github.io/assets/css/main.css
      " />
		<noscript><link rel="stylesheet" href="fivegoldstars.github.io/assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<h1>Full Report</h1>
						<p>How We Did It</p>
					</header>
				
				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="/">Back to Home</a></li>
							<li><a href="#context">Overview</a></li>
							<li><a href="#intro">The Project</a></li>
                            <li><a href="#review">Literature Review</a></li>
							<li><a href="#methods">Methodology</a></li>
							<li><a href="#data">Datasets</a></li>
                             <li><a href="#polar">Polar Chart</a></li>
                            <li><a href="#descr">Descriptive Analysis</a></li>
                            <li><a href="#statsan">Statistical Analysis</a></li>
                            <li><a href="#results">Results</a></li>
                            <li><a href="#limits">Limitations</a></li>
                            <li><a href="#ref">References</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

                        <!-- Context -->
                        
                        <section id="context" class="main">
                                <h2><b>Why this Project?</b></h2>
								<h3><b><u>Who cares about ratings?</u></b></h3>
                            
                            <p>As globalisation results in increased competition across products and services and our world becomes increasingly reliant on technology, society has put more and more faith in ratings to guide our decisions. With websites such as Amazon, Trivago or CompareTheMarket allowing consumers to compare everything from makeup to travel insurance at a glance, consumers put trust in peer-reviews and ratings in order to make informed purchases. Applications such as GoogleLens even allow users to view ‘reviews, address details and opening times’ (Hall & Boyle, 2019) of restaurants at a glance or look-up ‘reviews of movie DVDs on your entertainment stand’, making it easier than ever to simplify entire consumer experiences into numbers.</p>
                            
                            <p>Recently, the concept of ratings has expanded beyond the marketplace and entered the social realm. Apps like <em>Peeple</em> (<a href="https://www.forthepeeple.com/">1, </a><a href="https://itunes.apple.com/us/app/peeple/id1008896593?mt=8">2</a>)  allow users to ‘rate other human beings’ (McCarthy, 2016) according to their professional abilities, personal characteristics and dating life. Many have drawn parallels of the app to hte 'Nosedive' episode of <em>Black Mirror</em> where people are given a numeric rating based on their ‘interactions with other people’ (Connick, 2018) In 2016 when the episode first aired this seemed like nothing more than a dystopian reality, but apps like <em>Peeple</em> and <a href="https://www.businessinsider.com/china-social-credit-system-punishments-and-rewards-explained-2018-4?r=UK&IR=T">China's New Social Credit System</a> that sees citizens rewarded for good deeds that provide access to ‘cheaper public transport and shorter waits at hospitals’ (Connick, 2018) with low scores resulting in ‘slower internet speed’ and ‘restricted access to hotels’ prove society is getting closer to making fiction reality.
</p>
                                
                                <p>Then, considering ratings are gradually dictating countless aspects of our lives, it is natural to question exactly how much we can trust them. Does their accuracy justify our faith in them? This is what this research project aims to explore by looking at ratings in the context of films and film success. By comparing film reviews with their revenue we hope to discern how reliable ratings are in this particular domain in order to spark other research into whether our findings are applicable to other areas.
 </p>
                            
								<h3><b><u>For and Against Ratings</u></b></h3>
								<p>Although the above examples have evidenced the increasing presence of ratings in our lives, they are not necessarily a dominant variable when considering films. As society subconsciously becomes increasingly dependent on rating systems, films are consciously becoming increasingly politicised. 
                                    
                            <p>Recent films have begun to take on a political stance, incorporating this into their plot. For instance, films celebrating different ethnic minorities as a way of challenging Hollywood stereotypes (e.g. <em>Black Panther</em>, <em>Crazy Rich Asians</em>) became increasingly apparent in the box office. In fact, these films’ success is often accredited to ‘filmmakers who drew deeply from culturally specific experiences’ (Buckle, 2018) gaining ‘major community support.’ By creating films featuring ‘cultural-specific montages’ (Gray, 2019) that are ‘fundamentally about identity’, filmmakers are targeting a highly specific audience. Independent of ratings, their box office revenue may instead be attributed to their narrative and cultural context, appealing to an increasingly globalised audience.

                                        
                                        <p>Categorisation of films into genre (popularity of action films over documentaries) or even production studio (Marvel versus DC) may even be more indicative of success. For example, Marvel films receive ‘the most favorable reviews’ (Brueggemann, 2017) with a ‘small edge in overall popular appeal’, resulting in a larger revenue for the production studio. For these reasons, we have chosen to compare the correlation between ratings and revenue with other variables such as percentage male/female cast and genre. This should hopefully allow us to determine not only whether ratings are able to predict success, but also to what extent they are a contributing factor of success.
</p>
    
							</section>
                                
                                <!-- Introduction -->
							<section id="intro" class="main">
								<h2><b>The Project</b></h2>
                                <h3><b><u>Research Question</u></b></h3>
                                <p>Given the context described above and having decided to focus on film revenue, we generated the following question: <b>Can we trust public film ratings to predict how successful a film will be in the box office?</b>
                                </p>
                                
                                <h3><b><u>Aims</u></b></h3>
                                <p>Our project aims to examine various factors to determine the key variable in predicting box-office success.</p>
                                <p>The main goals for our project are to:</p>
                                <p><ul>
                                    <li> determine whether film ratings influence film revenued</li>
                                    <li> determine whether the gender of actors or film budget is more predictive of box-office success.</li>
                                </ul>
                                
                                <h3><b><u>Hypotheses</u></b></h3>
                                <p>The results we as a group expect to find from our analysis are:
                                <p><ul>
                                    <li> film ratings will not predict box-office success
.</li>
                                    <li>the gender of actors will predict box-office success</li>
                                <li>film budget will predict box-office success</li>
                                </ul>
                        </section>
                        
                        <section id="review" class="main">
                                <h2><b>Literature Review</b></h2>
                            
                            <p>Many studies have looked into both the various factors contributing to a film’s box office success (including those mentioned in the previous section) and also the effect that ratings and reviews have on consumer behavior and revenue.
                                
                                <p>Literature concerning the effect of reviews can be split into two categories: the effect of official reviews (given by professional critics) and amateur reviews (those of an average moviegoer). Research shows that although critical reviews do have some influence on gross earnings (King, 2007), they instead perform more as predictors of performance instead of root causes (Eliashberg & Shugan, 1997). Critics do not serve as opinion leaders for mainstream audiences and are instead indicators, with the correlation between reviews and cumulative box office takings for films not entirely conclusive (Eliashberg & Shugan, 1997). Research does to some extent indicate that the possible effects critical review has on box office success only occur after a certain amount of time, as majority tickets are purchased after most reviews have been published (King, 2007) resulting in a statistically insignificant relationship between critical reviews and box office revenues (Eliashberg & Shugan, 1997) until the fifth week after a film has been released. These findings suggest that although critical reviews are not a reliable way of predicting film success, they instead serve as a way of polarising films into either a ‘good’ or ‘bad’ category that ends up correlating with a film’s box office revenue. 
</p>
                            
                            <p>This may be because critics have a limited impact on consumer behavior (Austin, 1983), due to their difference in ratings as consumers have a tendency to evaluate films more positively than critics. Yet, research by Berg and Raddick (2016) found little correlation between IMDb user ratings and box office receipts, invalidating this as a reason for insignificant correlation between critical reviews and box office. Instead, it may suggest that ratings as a whole are a poor indicator of box office success. The results of our analysis may validate these findings when considering TMDb ratings, giving more credibility to the claim that ratings are not a reliable predictor of box office success. 
                                
                                <p>Research also demonstrates the effect of electronic word-of-mouth (e.g. peer reviews), with findings that oppose those by Austin, Berg and Raddick. A study in Russia (Gaenssle et al., 2018) shows that audience ratings had a significantly positive influence on box office revenue, with Brewer and Jozefowics (2009) echoing that good reviews from peer audiences result in higher gross revenue. Though these findings are contradictory, they may be explained by <em>IMDb’s</em> way of aggregating user rating. <em>IMDb</em> does not display average rating, but instead a weighted average to ‘eliminate and reduce attempts at vote stuffing’ (Help.imdb.com, n.d.). Whilst their exact formula to calculate weighted average is not publicly available, they seem to place emphasis on users who frequently rate films. It is also worth noting that only those registered to <em>IMDb</em> (likely movie enthusiasts) are allowed to rate films. This suggests that ratings shown on IMDb are not in fact representative of audience ratings, and are instead more reflective of critical ratings (or at least users with a tendency to engage with films on a deeper level through writing reviews than the average layperson). Therefore,  Berg and Raddick’s (2016) conclusion that audience reviews still do not predict box office success are false as their data still concerns critical reviews. If this is the case, our use of data from <em>TMDb</em> should show different results, as its main purpose is as a recommender system where users have little benefit to giving ratings other than to receive similar films they may like. However, the notion of film-enthusiasts (those not professionally employed to write critiques but with more knowledge than the average moviegoers) skewing ratings on all sites is still present and means there is no way for us to truly discern between critical and public opinion. </p>
<p>One of the other factors that has been used as an indicator of film success is budget, with big budgets seemingly indicating high revenues and large box office success (Gaenssle et. al, 2018, Ravid, 1999). Basuroy et al. (2003) go further to conclude that large budgets do well to positively affect box office performance for films with majority negative reviews, with little effect on those with majority positive reviews. These findings indicate that Basuroy et al. do perceive negative ratings have some effect on box office success, effects which can be negated by a film’s production budget. Brewer and Jozefowicz (2009) instead conclude that both production budget and favourable reviews have statistically significant effects on revenue. To what extent this is true can also be validated by our statistical analysis, comparing the correlation between ratings and revenue as well as budget and revenue. 
</p>
                            
                            <p>Variables that have also been attributed to influencing box office success are: the popularity of the cast and presence of ‘stars’ (Collins et al., 2002); the familiarity of the film genre (Desai & Basuroy, 2005, Brewer & Jozefowicz, 2009); the time of release (Brewer & Jozefowicz, 2009); a film’s ability to differentiate itself to meet consumer demand such as in the case of <em>Crazy Rich Asians</em> (Smith and Smith, 1986); or even search query patterns on Google (Google, 2013). Whilst interesting, these variables are not something our project can investigate in depth with the data from our chosen dataset. They are perhaps other factors that may explain our results, and will be looked at to some extent in our visual representation of the data but should instead act as springboards for further developed research into this area. 
</p>
                            
                            </b></h3>
                            
                        </section>
                        
                         <!-- Methodology -->
						
						<section id="methods" class="main">
								
                            <h2><b>Methodology</b></h2>
                            
                            <div id="toc_container">
<ul class="toc_list">
    <p class="toc_title">This section describes the research and presentation methods we used. The next paragraphs outline:</p>
    <li><a href="#visualisations">how we created our visualisations</a></li>
 <li><a href="#statsmethods">how we conducted our statistical analysis</a></li>
    <li><a href="#website">how we created our website</a></li>
</ul>
</div>
								
                            
                             <p>We measured the following film-features and contrasted them against film-revenue in turn:
                                <ul>
                            <li> film ratings
</li>
                            <li>film budget</li>
                            <li>film gender split</li>
                            </ul>

                            <section id="visualisations"><p><br><Br></p></section>
                            
                            <h3><b><u>How we created our visualisations</u></b></h3>
                            
                            <p>We created 3 sets of visualisations for each variable: bar-graphs, violin graphs and scatter plots.
</p>
                            
                            <p>Scatter plots were chosen as we could code a line of best fit which would show us if we should expect a positive or negative correlation between variables (or any relationship at all). They would also show us the density of the data i.e. if films were concentrated around a particular film rating. Bar-graphs were chosen as they show the distribution of data. The quantity of our data meant data points greatly overlapped in the scatter plots so having bar-graphs allowed us to categorise the data e.g. by rounding the budget, which in turn showed the separated distributions and frequencies more clearly. Finally, we used violin plots due to their ability to demonstrate the frequency distribution within variable categories rather than the frequency overall e.g. the distribution of film revenues within films which have 80% male actors.</p>
                            
                            <p>All these visualisations were created using Python Azure Notebooks and the Python library Seaborn.</p>
                            
                            <p>We also created one larger polar scatter chart to allow for a visualisation that showed our complete dataset, allowing us an initial insight into the data in an interactive way.</p>
                            
                            <h4><b><em>Bar-Graphs</em></b></h4>
                            
                            <p>We constructed bar-graphs because they are easy for readers to interpret. At a glance, the reader can see a large quantity of data and the changes between categories e.g. between film ratings. We also included additional bars to show the interquartile ranges.

 </p>
                             <p>We chose bar-graphs over line graphs due to the volume of data which made individual lines in the line-graphs difficult to see. Instead, we placed our data into categories which suits bar-graphs better.


</p>
                            
                            <div style=" font-size:80%; text-align:center; width: 100%;"><img src="http://fivegoldstars.github.io/images/Screenshot%202019-01-14%20at%2016.19.05.png"   style="padding-bottom:0.5em; max-width: 100%"  /></div>
                            
                           <div style=" font-size:80%; text-align:center; width:100%"><img src="http://fivegoldstars.github.io/images/Screenshot%202019-01-14%20at%2016.19.13.png"  style="padding-bottom:0.5em; max-height=450px"  /></div>
                            
                            <h4><b><em>Violin Graphs</em></b></h4>
                            
                            <p>We built violin graphs because, like bar-graphs, they are simple for readers to understand. Violin graphs display categorised data e.g. our weighted film ratings and also reveal more information than bar-graphs by presenting the probability density of variables e.g. how values within categories are distributed.

</p>
                            
                            
                            <div style=" font-size:80%; text-align:center; max-width: 100%"><img src="http://fivegoldstars.github.io/images/violingraphcode.png"   style="padding-bottom:0.5em; max-height:250px; max-width: 100%; "  /></div>
                            
                           <div style=" font-size:80%; text-align:center;"><img src="http://fivegoldstars.github.io/images/violingraphexample2.png"  max-height: 350px  style="padding-bottom:0.5em; max-width: 100%; max-height: 450px"  /></div>
                            
                            <h4><b><em><br>Scatter Plots</em></b></h4>
                            
                            <p>We coded scatter plots to see any initial correlations between budget / film gender-split / film rating and film revenue. We included a line of best fit to determine potential relationships between factors before undertaking the statistical analysis.

Scatter plots also showed us between which variables films were most concentrated./p>
                            
                            
                            <div style=" font-size:80%; text-align:center; max-width: 100%"><img src="http://fivegoldstars.github.io/images/scattercode.png"  height: 250px  style="padding-bottom:0.5em; max-width: 100%; max-height: 250px"  /></div>
                            
                           <div style=" font-size:80%; text-align:center;"><img src="http://fivegoldstars.github.io/images/scattergraph.png"  max-height: 350px  style="padding-bottom:0.5em;"  /></div>
                                
                            
                            <h4><b><em><br>Polar Chart</em></b></h4>
                            
                            <p>Our polar chart shows the distribution of films according to different film ratings with the colours representing different genres, the size of the points representing different budgets and the radius representing revenue.This will be looked at in greater detail in the Polar Chart section.</p>
                            
                               <section id="website"><p><br><Br></p></section>
                        
                            
                            <h3><b><u>How we created our website</u></b></h3>
                            
                            <p>Our website was built using a skeleton theme obtained from <a href="https://html5up.net/">HTML5 UP</a>. We chose Github as the hosting platform due to its ability to share files easily through a common repository, as well as the ability to upload Python workbooks to demonstrate our methods in greater detail. Using HTML to create our website (using the IDE Brackets) allowed for more flexibility in the layout of our site, although any changes had to be pushed to the repository causing a slight delay in comparison to the instant changes seen with platforms such as WIX. The ability to embed visualisations such as our Polar Chart with great ease was also an advantage to using Github over other platforms. </p>
                        
                                                    <section id="statsmethods"><p><br><Br></p></section>
                            
                            
                <p><h3><b><u>What statistical analysis methods we used</u></b></h3>

<p><h4><b>Spearman's Rank and Kendall's Tau</b></h4>

<p>We used Python to find the Spearman’s rank and Kendall’s tau correlation coefficients. This assigns a numerical value to the correlation and explains how strong/weak it is. From this information we will also know whether to accept or reject our hypotheses.
</p>

<p>Spearman’s rank is a non-parametric statistical test and is used instead of Pearson’s product-moment correlation when data is not normally distributed. It is used to determine the strength of a monotonic relationship (either when the value of one variable increases, so does the value of the other; or when the value of one variable increases, the other decreases) (Laerd Statistics, 2018).
</p>

<div style=" font-size:80%; text-align:center;"><img src="http://fivegoldstars.github.io/images/spearman-1-small.png"  height="height" style="padding-bottom:0.5em;" id="figure-extra-large" /><br><em>Figure A: example of monotonic and non-monotonic relationships</em></div>

<p>Coefficient values will be between -1 and +1. +1 denotes a perfect positive correlation, -1 denotes a perfect negative correlation and 0 denotes no correlation at all. It is calculated using the following equation:</p>

<div style=" font-size:80%; text-align:center;"><img src="http://fivegoldstars.github.io/images/Figure%208.5.png"  max-width: 250px height="height" style="padding-bottom:0.5em;" id="figure-small" /><br><em>In the equation <b>d</b> is the difference between ranks and <b>n</b> is the number of ranks</em></div>


<p><br>Kendall’s tau is a non-parametric test similar to Spearman’s Rank. It is used when many scores have the same rank e.g. for our film ratings (Field, 2005).</p>

<p><b><h4>Spearman's Rank vs. Kendall's Tau</b></h4></p>

<p>Interpretations of the two tests are very similar but Spearman’s is more widely used. Kendall’s tau p-value is more accurate if there is a smaller sample size which is why we also calculated Spearman’s rank. However, there is evidence to suggest that Kendall’s tau is a better estimate of correlation than Spearman’s rank(Field, 2005). Kendall’s tau is also much less sensitive to error. Furthermore, Kendall’s tau is calculated based on comparing pairs of data points and seeing if they ‘match’, whereas Spearman’s rank is based on deviations (Statistics Solutions, 2018).</p>

<p>For our project we therefore chose to use both given that for some variables, many scores had the same rank but for others they did not. Using both tests will also pick up differences in accuracies between the two and strengthen overlaps in results.
    
                             
                        
                        </section>
                            
                            
                            <section id="data" class="main">
								<h2><b>Datasets</b></h2>
                                
<div id="toc_container">
<ul class="toc_list">
    <li><a href="#source"> The source of our data </a></li>
    <li><a href="#reliability"> Reliability</a></li>
    <li><a href="#cleaning">What we did </a></li>
    <li><a href="#ethics">Ethics </a></li>
 
</div>
                                
                                <section id="source" class="main"><br><br></section>
                                
                                <h3><b><u>The source of our data</h3></u></b>
    
    <p>Our data came from <a href="https://www.kaggle.com/"><em>Kaggle</em></a>, a platform where users can upload datasets for other users to download and analyse. Our specific dataset can be found here a href="https://www.kaggle.com/rounakbanik/the-movies-dataset/home"><em>The Movies Dataset</em></a>.  </p>
                            
                            <p>We chose this to be the source of our data as not only did it contain the information we required for our analysis (movie budget, revenue, credits) but it was in a machine readible format (.csv) making it perfect for immediate use. The file contained metadata for the 45,000 movies listed on <a href="movielens.org"><em>MovieLens,</em></a> with ratings being recorded from <a href="themoviedb.org"><em>TMDb.</em></a> 
                                
                                <p>The dataset collected data through using the <em>TMDb</em> Open API whereas the ratings were compiled from the <em>MovieLens</em> datset found <a href="https://grouplens.org/datasets/movielens/latest/">here.</a> The fact that these two sources of data had already been cohesively compiled meant that less time needed to be spent gathering data and could instead be used for analysis.</p>
                            
                            <p>This dataset also gave us a large initial amount of data to work with (although the number of movies was narrowed it down through selective filtering processes described later) which would allow our analysis to hopefully be more reflective of the entire film industry as we were unable to obtain data for every film produced. 
                            
                            </p>
                                
                                 <section id="reliability" class="main"><br><br></section>
                                
                                <h3><b><u>Reliability</h3></u></b>
                            
                            <p>The reliability of the data is somewhat unclear. Whislt the dataset itself obtains data from two established websites, both of these websites are built on user-input.</p>
                            
                            <p><em>TMDb</em> is a 'community built' (TMDb, n.d.) database, with every piece of data available having been contributed by a user. Although the site is facilitated by staff moderators and recommendations are given for where to obtain information such as film revenue or budget, there is no way for us to validate all 45,000 entries. Instead, we must put trust in TMDb's authentication process. This also means that not every film has a complete set of metadata as shown during our cleaning process, narrowing down the number of useable films by a considerable amount. The <em>TMDb</em> site describes itself as a 'trusted platform' (TMDb, n.d.) but as we have no means of inquiring how they obtain their data or how rigorously they check the accuracy of new information; we cannot declare this source as 100% reliable.</p>
                            
                            <p>We decided not to use the separate ratings file (ratings.csv) that collated <em>MovieLens</em> ratings for individual movies, and instead use the ratings given by <em>TMDb</em> users in the movies_metadata.csv file. This was because the amount of votes one each film was larger than the amount on <em>MovieLens</em> which we hoped would lead to more representative ratings for us to analyse. How reliable these ratings are is not only the very problem our project is trying to solve, but also somewhat impossible. We rely on users to have given an accurate rating for their opinion of the film, but have no way of verifying this. As <em>TMDb</em> is a community site as opposed to <em>Rotten Tomatoes</em> where official critics can give reviews, it may be suggested that <em>TMDb's</em> ratings are not as 'esteemed' as other sources. However, given that this project aims to evaluate ratings as a whole (not only reviews given by professionals) this seems appropriate for our research.</p>
                                
                                 <section id="cleaning" class="main"><br><br></section>
                                
                                <h3><b><u>What we did</h3></u></b>
    
    <h4><b><em>Cleaning</em></b></h4>
    
    <p>The first step in cleaning the data was removing unnecessary columns like runtime or production country in order to create a dataframe more focussed for our needs. Then, we removed from our dataframe every movie with missing quantitative information because for sufficient analysis we needed complete information for all variables in order to gain a complete view of the data. In addition we filtered out films that have yet to be released. 
</p>
    
    <p>This process reduced the number of usable films from 45000 to 5023. </p>
    
<h4><b><em>Merging</em></b></h4>
    
    <p>Next, we merged the three separate .csv files (movies_metadata, credits and ratings) into one dataframe. We did this by merging on the movie id (a unique number assigned to each movie). </p>
    
    <h4><b><em>Improving Readability</em></b></h4>
    
    <p>In this step we removed unnecessary formatting in the columns to make them more readable, both for the programme and the humans working with it. For example, the cast column contained information on the name of the characters and links referring to the actor’s IMDb page. As we were only interested in the gender of the cast we removed the excess data as well as removing the stylistic parentheses and inverted quotes.  We then ended up with the genders of cast members represented as numbers with 2 representing male, 1 representing female and 0 representing unknown (information not logged on TMDb).  
</p>
    
    <h4><b><em>Calculating Male/Female Ratio</em></b></h4>
    
    <p>As percentage male/female cast was a variable we wanted to analyse, we needed to calculate this information as it wasn’t present in the dataframe. To do this, we first added new columns for the number of males, females and unknown and the size of the cast. We then created a for loop going through each row and counting how often a 2, 1 or 0 appears and stored this information in the newly created columns. The size of the cast was the length of the array in the cast column.  

We then decided to remove movies that had ‘unknown’ cast members. This was because if a movie had for example 2 male and 3 female actors, but 6 unknown there is not sufficient information to accurately estimate the gender split. For the remaining movies we added columns with the percentages of male and female actors by dividing the number of males/females in a film by the overall cast size. 
</p>
    
    <h4><b><em>Weighted Rating</em></b></h4>
    
    <p>We decided to calculate a weighted rating for each movie because of the big differences in vote count for some movies. ‘Weighted’ refers to the fact that some votes are given a greater weight, for example if two movies have a rating of 5.4 but one movie has 14 votes and the other 340, the rating of the first movie would have less significance on the overall average.  
</p>
    
    
    <h4><b><em>Problems</em></b></h4>
    
    <p>The biggest obstacle we faced was during the first part of cleaning the data.
During the merging phase, we found that Python wasn’t reading the datatypes of each column correctly and was instead reading columns containing numbers (such as movie ID) as strings, which made the data practically unusable when trying to write appropriate code. To combat this, we had to re-establish the datatype of the ID column as integers. Moreover, after successfully merging the data frames, we noticed that the overall number of movies actually went up. After investigating through various parts of the data frame, we noticed that the merging produced some duplicates, which needed to be deleted.
</p>
                                
                                 <section id="ethics" class="main"><br><br></section>
                                
                               <h3> <b><u>Ethics</h3></u></b>
    
    <p>Even though the datasets we used are publicly available to everyone, it is important to consider ethical implications especially in regard to the source of the data. The dataset was published on <a href="http://kaggle.com"> <em>Kaggle</em></a>, with this particular dataset being a mixture of data collected from <a href="http://themoviedb.org"><em>TMDb</em></a><a href="http://themoviedb.org"> and <a href="http://grouplens.org"><em>GroupLens<a href="http://grouplens.org"></em></a>. In terms of numbers for budget and revenue the data comes from the TMDB Open API (<em>TMDb</em> is a community led website). A look into their user guidelines reveals that they recommend the websites <a href="http://boxofficemojo.com"><em>BoxOfficeMojo</em></a> and <a href="http://the-numbers.com"><em>The Numbers</em></a> as sources for budget and revenue data.
</p>
    
        <p><em>The Numbers</em> is directed by <em>Nash Information Services, LLC</em> “the premier provider of movie industry data and research services.” Because of their close link with industry professionals we can assume this data has been acquired ethically and legally, and not through alternative means such as an invasion of privacy as seen with the Sony Pictures Hack in 2014. 
</p>
    
    <p>In terms of the ratings and vote count we decided to only use data from <em>TMDb</em> users and not from GroupLens research because the latter got the data from an API on the website <em>MovieLens</em> which is less popular than  <em>TMDb</em>. The  <em>TMDb</em> website is operated by <em>Fanhattan Inc</em>. In order to rate films, a user has to register with their email and create a username. In their Privacy Policy, <em>Fanhattan Inc.</em> states that they collect data from their users such as location, favourite movie genres or searches, which becomes associated with the personal information provided by the user. However before they share this data with third parties or use it for example by tracking usage patterns or performing analytics, they “create anonymous data records from [the user’s] personal information ("Anonymous Data") by excluding information (such as [the user’s] name) that makes the data personally identifiable to [the user].” All the data we used was therefore anonymized and didn’t allow for any conclusions on the identity of the users. The only names stated are of the actors and crew members of the films, which is publicly available data. 
</p>
								
							</section>
        
         <!-- Polar Chart-->
			
                
                <section id="polar" class="main">
                                <h2>Polar Chart</h2>
                    
                    <iframe width="100%" height="700" frameborder="0" scrolling="no" src="//plot.ly/~qm2group11/85.embed"></iframe>
                    
                    <h3><b><u>Why we made the chart </u></b></h3>
                    
                    <p>In order to display all our data in one visualisation and make initial, general comments we decided to create a polar chart. This visualisation is interactive, allowing us to clearly display all 5023 data points that readers can explore through looking at different sections of the chart. Whilst the limitations for this visualisation are that no statistical analysis can be performed on it so the results are not conclusive, it gives a good starting point for understanding the nature of our data; with more in-depth statistical analysis following in the next section. It also allowed a good display of all data points without appearing as cluttered as a Bubble Graph. 
</p>
                    
                    <p>The graph was created using plot.ly’s online chart studio and can be viewed in greater detail <a href="https://plot.ly/~qm2group11/85">here</a>. 
</p>
                    
                    <h3><b><u>How to interpret the chart</u></b></h3>
                    
                    <p>Four different variables are represented on the polar chart: average rating, box office revenue, genre and film budget. Theta represents the average rating (with each rating multiplied by 36 to scale to the 360 axes, e.g. a film with an average rating of 7 is represented with theta 252); radius represents box office revenue in dollars; the colour of the data point represents the genre of film as described by the legend on the left;  and the size of the data point corresponds to the size of the budget. 
</p>
                    
                    <p>Hovering over a datapoint gives you the title of the film, the radius, theta value, and genre. To filter the data according to revenue, place the cursor at the lower limit of revenue you want and drag outwards to the upper limit of revenue. For example, if you wanted to see all films with a revenue of above 2 billion dollars but below 2.5 billion dollars, you would click and drag from the 2 billion dollar radial outline outwards to the 2.5 billion dollar radiar line. To zoom out again click twice on the chart. 
</p>
                    
                    <h3><b><u>Initial thoughts</u></b></h3>
                    
                    <p>As previously mentioned, no particular statistical analysis can be performed on this graph but there are some clear initial findings: </p>
                    
                    <ul>
                    
                    <li>most of the highest grossing films (as these are the only ones that can be seen when the chart is at its default zoom setting) have a rating between 5 and 9
</li>
                    <li>the highest grossing films appear to have ratings between 8 and 9 which suggests at least that the most successful films of our dataset have good ratings, but this correlation may not be true throughout the entire scope of our data
</li>
                    <li><em>Avatar</em> is the highest grossing film in our dataset, but does not have the highest rating nor the largest budget, suggesting that the three variables are not mutually dependent
</li>
                    <li>amongst the highest grossing films, the Action and Science Fiction genres at a glance appear to be the most present, however as this is visual analysis it is impossible to apprehensively draw any conclusions. This does however support Brewer & Josefowicz (2000)’s results suggesting there is a correlation between genre familiarity and box office success. 
</li>
                    
                    </ul>
                    
                    <h3><b><u>Thoughts from literature review</u></b></h3>
                    
                    <p>One benefit of the polar chart is it allows for direct analysis between films. Although this does not help establish overall trends, it does allow shallow investigations to the claims made in the ‘For and Against Ratings’ section of the report, as well as validity of the research discussed in the Literature Review. 
</p>
                    
                    <p>Whilst neither <em>Crazy Rich Asians</em> or <em>Black Panther</em> appears in our cleaned data, <em>Joy Luck Club</em>, a 1993 film featuring a majority Asian-American cast, is similar in terms of being focussed on one ethnic group and promoting a specific cultural identity. Although it premiered in a different cultural climate, as shown in the image below in comparison to other drama films with a similar revenue, it had a better rating. This may be due to it being a culturally differentiated product, scoring higher ratings due to its perceived ‘difference’ in comparison to other films at the time. Whilst its box office revenue is not drastically high, it may be interesting in future to plot its success along similar films from 1993 with a majority white cast. </p>
                    
                 <div style="width:100%">
<div class="row">
  <div class="column" style="font-size:80%; text-align:center; align-content: center">
      <img src="http://fivegoldstars.github.io/images/data1.png" style="max-height:350px; max-width: 100%; "  align="middle"/><br>
  </div>
  <div class="column" style="font-size:80%; text-align:center">
    
    <img src="http://fivegoldstars.github.io/images/data2.png" style="max-height:350px; max-width: 100%; " align="middle"/><br></div>
</div>
    </div>
                    
                    <p>When trying to compare Marvel vs DC studios (as this is information is not contained in the clean dataset) at a glance, it does appear that Marvel films in general perform better at the box office. For instance, DC’s <em>Dark Knight</em> scores a higher rating than <em>Avengers: Age of Ultron</em>, but <em>Age of Ultron</em> had a higher revenue. Another interesting thing to note is that <em>Guardians of the Galaxy 2</em> had higher revenue than <em>Guardians of the Galaxy</em> but scored a lower average rating, perhaps linked to Brewer and Jozefowicz (2009)’s other conclusion that a successful film prequel creates a higher revenue for subsequent films. 
                        
</p>
                    
                    <div style="width:100%">
<div class="row">
  <div class="column" style="font-size:80%; text-align:center; align-content: center">
      <img src="http://fivegoldstars.github.io/images/data4" style="max-height:350px; max-width: 100%; "  align="middle"/><br><em>Figure 4: Frequency table for weighted rating</em>
  </div>
  <div class="column" style="font-size:80%; text-align:center">
    
    <img src="http://fivegoldstars.github.io/images/data3.png" style="max-height:350px; max-width: 100%; " align="middle"/><br><em>Figure 5: Descriptive statics for weighted rating</em></div>
</div>
    </div>
                    
                    <div style="width:100%">
<div class="row">
  <div class="column" style="font-size:80%; text-align:center; align-content: center">
      <img src="http://fivegoldstars.github.io/images/data5.png" style="max-height:350px; max-width: 100%; "  align="middle"/><br><em>Figure 4: Frequency table for weighted rating</em>
  </div>
  <div class="column" style="font-size:80%; text-align:center">
    
    <img src="http://fivegoldstars.github.io/images/data6" style="max-height:350px; max-width: 100%; " align="middle"/><br><em>Figure 5: Descriptive statics for weighted rating</em></div>
</div>
    </div>
                    
                    <p>All in all, the context of a film may be important which is something this visual representation is missing. Whilst the polar chart is useful for comparing films in a case by case basis, the descriptive analysis in the following section helps establish a more general picture. That being said, there may be a story the data can’t tell, such as whether or not <em>Joy Luck Club’s</em> revenue was due to good narrative elements or audience reception. While the polar chart is a good way to display all of our data at once, we must still rely on statistical methods to calculate whether or not there is any correlation between the different variables. 
</p>
							</section>


<!--Descriptive Analysis-->
        
        <section id="descr" class="main">
            
            <h2><b>Descriptive Analysis</b></h2>
            
            <div id="toc_container">
<ul class="toc_list">
  <li><a href="#overalldata"><b>1. Analysis on the overall datasets</b></a>
  <ul>
    <li><a href="#revdistribution">Distribution of Revenue</a></li>
    <li><a href="#ratingdistribution">Distribution of Rating</a></li>
  </ul>
</li>
 <li><a href="#relationships"><b>2. Analysis on relationships</b></a>
  <ul>
    <li><a href="#revratings">Revenue and Ratings </a></li>
    <li><a href="#revgender">Revenue and Gender</a></li>
  </ul>
</li>
</div>

<p>This section summarises our data systematically and uses it to evaluate the relationships set out in the hypotheses. We first look at how the overall datasets behave by doing distribution analysis. We then visualise the data using tables and graphs, and relate this back to our hypotheses.


</p>

                <section id="overalldata" class="main"><br><br></section>
                
                <p><b><u><h3>1. Analysis on the overall datasets</h3></u></b></p>

<p>We first looked at the distributions of film revenue and ratings to understand how much films usually earn and how well users rate them . Frequency tables, descriptive statistics and histograms were used for the analysis (SAS Institute Inc., 1999).</p>

                <section id="revdistribution" class="main"><br><br></section>
                
                <p><b><h4><em>Distribution of Revenue:</h4></b></em></p>

<p>We constructed Figure 1, Figure 2 and Figure 3  to look at the distribution analysis of the film revenue.</p>

<p>The first row of Figure 1 shows that 1,4302 films lie on the mean interval of 100 million, suggesting their revenue ranges from $0 to $200 million. This implies that a majority of films, precisely 86.7% of the total films collected for this analysis, obtained a revenue within this range. The average revenue for the films is $146 million, with a minimum of $1 and maximum of $2788 million, according to the Revenue Statistics in Figure 2.
</p>
                    

<div style="width:100%">
<div class="row">
  <div class="column1" style="font-size:80%; text-align:center; align-content: center">
      <img src="http://fivegoldstars.github.io/images/newfreq.png" style="max-height:350px"  align="middle"/><br><em>Figure 1: Frequency table for revenue</em>
  </div>
  <div class="column2" style="font-size:80%; text-align:center">
    
    <img src="http://fivegoldstars.github.io/images/Screenshot%202019-01-14%20at%2011.01.51.png" style="max-height:350px;" align="middle"/><br><em>Figure 2: Descriptive statistics table for revenue</em></div>
</div>
</div>

<p><br>In addition, we plotted the frequency table graphically into a histogram, as shown in Figure 3.  

</p>

<div style="width:image width px; font-size:80%; text-align:center;"><img src="http://fivegoldstars.github.io/images/hist2.png"  style="padding-bottom:0.5em; max-height:350px; " /><br><em>Figure 3: Histogram showing frequency for revenue</em></div>
                    <br>

<p><br>Histograms can indicate skewness and are often used to test how far and in which direction a skew is deviated from the horizontal symmetry of a normal distribution (Brownmath, 2016). The histogram above demonstrates a positive skewness of 5.34, which is defined as highly skewed (Brownmath, 2016). This will also be used for the statistical analysis in the next section.


</p>

                <section id="ratingdistribution" class="main"><br><br></section>
                
                <p><em><b><h4>Distribution of Rating:</em></h4></b></p>

<p>We constructed Figures 4, 5 and 6, for the distribution analysis of the film rating.

</p>

<p>As displayed on the table on Figure 4 , 2904 films lie on the mean interval of 5.5, which suggests their ratings range from 5 to 6; 1825 films lie on the mean interval of 6.5, which suggests their ratings range from 6 to 7. This indicates that the majority of films, precisely 95.3% of the total films collected for this analysis, received ratings in these 2 ranges. The average rating for the films is 6.08, with a minimum of 4.8 and maximum of 8.25, based on the data from Rating Statistics on Figure 5.</p>
                    

<div style="width:100%">
<div class="row">
  <div class="column1" style="font-size:80%; text-align:center; align-content: center">
      <img src="http://fivegoldstars.github.io/images/frequencytableforweightedrating.png" style="max-height:350px; max-width: 100%; "  align="middle"/><br><em>Figure 4: Frequency table for weighted rating</em>
  </div>
  <div class="column2" style="font-size:80%; text-align:center">
    
    <img src="http://fivegoldstars.github.io/images/descriptivestats2.png" style="max-height:350px; max-width: 100%; " align="middle"/><br><em>Figure 5: Descriptive statics for weighted rating</em></div>
</div>
    </div>

<p>The histogram below on Figure 6 demonstrates a positive skewness of 0.884, which is defined as moderately skewed (Brownmath, 2016). This again will also be used for the statistical analysis in the next section.




</p>

<div style="width:image width px; font-size:80%; text-align:center;"><img src="http://fivegoldstars.github.io/images/histogram1.png"  style="padding-bottom:0.5em; max-height:350px;" /><br><em>Figure 6: Histogram showing frequency for weighted rating</em></div>

<p><br>However, there are errors in the data table as data is missing for 62 films. This may have affected the results of the analysis.


</p>
                
                <section id="relationships" class = "main"><br><br></section>

<p><b><u><h3>2. Analysis on relationships</b></h3></u></p>

<p>This section will test our central hypothesis by determining the relationship between film ratings and their corresponding film revenue. We also looked at other relationships, including gender of actors in films and revenue, as well as film budget and revenue.

</p>

<p>For the analysis of revenue and ratings, we used cross tabulation to determine the relationship between the two variables in depth. For the analysis of each of the 3 combinations, we examined the relationship visually using scatter plots, bar-graphs and violin graphs.  All graphs were plotted with the horizontal axis being film rating, film gender-split and film budget respectively, and the vertical axis being film revenue in billions. This is because we consider film revenue to be the dependent variable and rating, gender, budget to be the independent variables (Cleveland, 1993). For the graphs where the independent variable is categorical, including bar charts and violin graphs, the film revenue has been converted to average revenue for each interval on the horizontal axis to be able to construct the graph. Since the films under the same interval on the horizontal axis are generalised into one part, the revenue they obtained also has to be integrated.

</p>

                <section id= "revratings" class="main"><br><br></section>
                
                <p><em><b><h4>Revenue and Ratings</h4></b></em></p>

<p>Cross tabulation is a table that summarises the frequency of variables in each table cell with defined features. For instance, as shown in Figure 7, films in the cell of rating 4-5 and a revenue of $1 to $200 million have a frequency of 2. Moreover, in this particular cross tabulation, the percentages of the frequency of the film out of the total frequency of the group of film with 1 shared feature have also been shown. For example, for all the films rated 4-5, there are 66.7% of them that had a revenue between $1 to $200 million.</p>

<p>For our analysis, by displaying how many films are distributed in each rating-revenue cell, our cross tabulation can allow viewers to see the relationship between film ratings and revenue at a glance. There exists some patterns in the table. First of all, most films rated 5-6, specifically 94.1%, received a revenue of $1 - $200 million, as highlighted in the table on Figure 7. Followed by films rated 6-7, which takes up 80.5% of the same revenue category. In addition, films with relatively low ratings (below 6) did not have high revenues, which evidences that film revenue might be statistically dependent on ratings. On the other hand, the data demonstrates the that films with very high ratings do not necessarily have very high revenues. For instance, for films rated 8-9, none of them obtained revenue above $1300 million. As a contrast, some films rated 7-8 obtained revenue of this range. This shows evidence to hypothesis number one.</p>

<div style="width:image width px; font-size:80%; text-align:center;"><img src="http://fivegoldstars.github.io/images/weightedratingtab.png"  style="padding-bottom:0.5em; max-height:500px;" /><br><em>Figure 7: Revenue * Weighted Rating Crosstabulation</em></div>

<p><br>Yet, we considered that the analysis could be biased due to how the data was divided into intervals and the definition of what is considered high and low revenue as well as high and low ratings. For instance, since most films obtained a revenue below $200 million, it might not be effective to divide the data evenly within the revenue range of all the films selected and  make $1 - $200 million as the lowest revenue category. To eliminate this distribution effect, we conducted another cross tabulation for films below $200 million, as displayed below in Figure 8.</p>
    
<div style="width:image width px; font-size:80%; text-align:center;"><img src="http://fivegoldstars.github.io/images/var_ratingvar.png" style="padding-bottom:0.5em; max-height:350px; max-width: 100%" /><br><em>Figure 8: var * rating var Rating Crosstabulation</em></div>

<p><br>Similar to Figure 7, most films selected received ratings of 5-6 and 6-7 and obtained relatively low revenue as shown in the top red box in this table. Interestingly, films with rating 6-7 consistently received higher revenue than films with rating 5-6, as highlighted in the lower red box on the left, except for the revenue interval of $170 million. Furthermore, very few films were rated above 7, and not many of them received high revenue. These results connote that although the data demonstrates the relationship that higher ratings of films does not indicate higher box office revenue, there is evidence showing that films with relatively but not very high ratings obtained higher box office revenues than films with lower ratings.</p>

<p>The data for revenue and film ratings have also been displayed graphically for more comprehensive analysis. The graphs are plotted with the horizontal axis being film rating and the vertical axis being film revenue in billions. The results from the graphs demonstrate a weak correlation between film revenue and rating. Firstly, as shown on the scatter-plot on Figure 9, the flat best-fit line implies a weak correlation between the 2 variables. This will be illustrated further in the statistical analysis section. In addition, some films with higher ratings do demonstrate higher revenues, but only very few of them, leading to a less robust result. Specifically, the films with high revenue and ratings can be identified as the isolated dots on the graphs, especially ones above $1.5 billion revenue. They were all rated between 6 and 7.5. The number of these films is small as reflected by the low density of the dots, compared to the ones scattered densely around the line of best-fit, below the $1 billion revenue. At the same time, higher ratings don’t alway indicate high revenue. For instance, the films rated above 7.5 are across all ranges of revenue, and a larger proportion of them lie in the lower revenue range below $0.5 billion revenue compared to the ones above as indicated by the density of the dots. The few films rated above 8 also received revenue below $0.5 billion.</p>

<div style="width:image width px; font-size:80%; text-align:center;"><img src="http://fivegoldstars.github.io/images/ratingrevenue.png"  style="padding-bottom:0.5em; max-height:350px;" /><br><em>Figure 9: Scatter graph showing correlation between film revenue and film rating</em></div>

<p><br>The results from the bar-charts also exhibit a weak relationship between film revenue and rating. Firstly, as shown on Figure 10, the bars form a fluctuating upward sloping trend. For instance, the average revenue increases from rating 7.2 to 7.3, decreases drastically from 7.3 to 7.4 and reaches a precedence at rating 8.0. Similarly, on Figure 11 shows the effect for each individual rating, rounded. Now a increasing trend is more noticeable, despite the outlier of revenue for the rating 6.</p>
            
<div style="width:100%">
<div class="row">
  <div class="column" style="font-size:80%; text-align:center; vertical-align: middle">
      <img src="http://fivegoldstars.github.io/images/ratingbarchart1.png" style="max-height:350px; vertical-align: middle; max-width: 100%" /><br><em>Figure 10: Bar chart showing the distribution of ratings and revenue</em>
  </div>
  <div class="column" style="font-size:80%; text-align:center; vertical-align: middle">
    
    <img src="http://fivegoldstars.github.io/images/ratingbarchart2.png" style="height:350px; vertical-align: middle; max-width: 100%" /><br><em>Figure 11: Bar chart comparing revenue and films with a rating of 5 to 8</em></div>
</div>
</div>

<p><br> The violin chart below on Figure 12 visually reveals the distribution of revenue within each film rating. According to the graph, most films have revenue below $0.5 billions and the highest revenue appeared in films rated 7.</p>

<div style="width:image width px; font-size:80%; text-align:center;"><img src="http://fivegoldstars.github.io/images/ratingviolin.png"  height="450px" style="padding-bottom:0.5em; max-width: 100%" /><br><em>Figure 12: Violin chart showing the distribution of revenue within each film rating </em></div>

                <section id="revgender" class="main"><br><br></section>
                
                <p><em><b><h4>Revenue and Gender</h4></b></em></p>

<p>The graphs for this section are plotted with the horizontal axis being percentage of male actors in films and percentage of female actors in films respectively, and the vertical axis being average film revenue in billions. As shown below, the male version and the female version are symmetrical. This is because for each film the percentage of male and female adds up to be 1, resulting in complementary distributions. For instance, films where 20% of actors were males have 80% of actors who are female.</p>

<p>The scatterplots below demonstrate the trend that films that have moderately high proportions of male actors correspond to higher revenue. Specifically, in Figure 13, there exists an increasing trend in revenue for films where the percentage of male actors is between 0% and 80%. This can be reinforced by the increased height of the densely scattered points near 0.4 revenue as well as the upward sloping direction of the distantly scattered points above 0.6 revenue. The film where the percentage of male actors is 80% obtained the highest average revenue. However, the trend starts to reverse above 80%, indicating that percentage of male actors being too high is not favorable for the film revenue success. This is the opposite for the scatter-plot for revenue and percentage of female actors on Figure 14, due to the nature of symmetry.</p>
            
<div style="width:100%">
<div class="row">
  <div class="column" style="font-size:80%; text-align:center; vertical-align: middle">
      <img src="http://fivegoldstars.github.io/images/malereg.png" style="height:350px; vertical-align: middle; max-width: 100%" /><br><em>Figure 13: Scatter plot showing the relation between revenue and percentage of male actors</em>
  </div>
  <div class="column" style="font-size:80%; text-align:center; vertical-align: middle">
    
    <img src="http://fivegoldstars.github.io/images/femalereg.png" style="height:350px; max-width: 100%; vertical-align: middle" /><br><em>Figure 14: Scatter plot showing the relation between revenue and percentage of female actors </em></div>
</div>
</div>
    
<p>The data for revenue and budget have also been plotted with the horizontal axis being film budget and the vertical axis being film revenue in billions. The revenue for the barcharts and violin charts is average revenue.
</p>

<p>As shown on the scatterplot in Figure 19 below, there is a conspicuous positive correlation between revenue and budget. This can also be illustrated by the relatively steep best-fit line that is closer to the y=x function with a slope of 1 than to the horizontal axis.
</p>

<div style="width:image width px; font-size:80%; text-align:center;"><img src="http://fivegoldstars.github.io/images/budgetreg.png"  height="350px" style="padding-bottom:0.5em;" /><br><em>Figure 15: Scatter plot showing the correlation between film budget and film revenue </em></div>

<p><br>Although there are some outliers above the budget of $250 million, as indicated by the points below the best-fit curve and the few ones lie within the budget of $300 to $400 million. The strength of the correlation might have been weakened by the high number of films that have low revenue and low budget, as indicated by the densely scatter points in the lower left hand corner of the graph.
</p>
<p>Our descriptive analysis can be concluded into four parts. Firstly, to summarise the distribution of the overall data, most films lie in the category of low revenue and low ratings compared with the overall range of revenues. Secondly, to summarise the relationship between film revenue and rating, our data has demonstrated that low rated films received lower revenue, and films with moderately high ratings have revenue higher than films with lower ratings.  Yet, it does not conclusively show that high rated films receive higher revenue. Thirdly, to summarise the relationship between film revenue and gender, films with moderately high percentage of male actors received higher revenue than those with high percentage of female actors. However, a very high percentage of male actors is not favorable for generating high revenue. Last but not least, to summarise the relationship between film revenue and budget, there is a positive correlation between film revenue and budget, although this is subject to the extreme case where very high budget actually corresponds to a lower revenue. This may be because a higher amount of revenue is needed for the film to turn a profit.</p>
            
</section>
                

                              
                
                 <!--Statistical Analysis-->
                
<section id="statsan" class="main">
<h2><b>Correlation Analysis</b></h2>

    
<div id="toc_container">
<ul class="toc_list">
    <li><a href="#statsintro">Introduction</a></li>
    <li><a href="#revsratings">Revenue vs Ratings</a></li>
     <li><a href="#revsbudget">Revenue vs Budget</a></li>
     <li><a href="#revsmale">Revenue vs Gender (Male Actors)</a></li>
    <li><a href="#revsfemale">Revenue vs Gender (Female Actors)
    </a></li>
    <li><a href="#statsconclusion">Conclusion
    </a></li>
 
</div>
    
                
    <section id="stastintro" class="main"><br><br></section>
    
    <h3><b><u>Introduction</u></b></h3>
    
<p>Correlation analysis depends to some extent on making assumptions about the collected data and choosing the most suitable way of comparing that data. The performance of the correlation analysis methods depends on the form of the analysed data and how it relates to the approach being used. There are parametric and nonparametric test methods. Parametric ones, such as Linear Regression and Pearson Correlation Coefficient, assume that sample data comes from a population that follows a probability distribution based on a fixed set of parameters. In contrast, a nonparametric test, such as Kendall’s Tau and Spearman’s Rank Correlation Coefficient, is suitable for data for which the parameter set is not fixed. Nonparametric tests work for data which does not follow any distribution or has a specified distribution but with unspecified parameters.
</p>
    
<p>Relying on a fixed parameter set, parametric models assume more about a given population than nonparametric methods do. When the assumptions are correct, parametric methods produce more accurate estimates compared to nonparametric methods. However, as more is assumed by a parametric test, when the assumptions are not correct, there is a bigger risk of them failing.

</p>

<p>In this project, the sample data collected on revenues, rankings, gender and budget is data with no specified distribution. Therefore, only nonparametric tests could be used to determine the correlation between revenue and 3 different variables (user ratings, gender of cast and budget). The two accepted tests of nonparametric rank correlations are Kendall’s tau and Spearman’s Rank Correlation Coefficient (Spearman’s rho). These two tests assess statistical associations based on the ranks of the data to measure the strength of the relationship between two variables. In most of the situations, the interpretations of Kendall’s tau and Spearman’s rank are very similar and, thus, lead to the same conclusions.
</p>
    
    <p>As stated in previous sections, there a few minor differences between the two tests which make them suitable for various datasets. Compared to Spearman’s rank correlation, Kendall’s tau produces usually smaller values, it is insensitive to error and its p-values are more accurate with smaller sample sizes. While Kendall’s tau’s calculations are based on concordant and discordant pairs, Spearman’s rank’s calculations are based on deviations.


</p>
    
    <p>Since samples of revenues, rankings, gender and budget vary in size and distribution (and possibly in some other ways), both tests were conducted to pick up differences in accuracies between the two and strengthen reliability of conclusions made after verifying results.
</p>
    
    
    <p>To conduct Spearman’s rho, the following conditions must be satisfied:<ul>
<li>the two variables should be measured on an ordinal, interval or ratio scale,</li>
<li>there is a monotonic relationship between the two variables.</li>
</ul>
    
    <p>To conduct Kendall’s tau, the following conditions must be satisfied:
<ul>
<li>the two variables should be measured on an ordinal or continuous scale,
</li>
<li>there is a monotonic relationship between the two variables.
</li>
</ul>
    
    <p>Datasets collected in this project (revenue, ratings, gender, budget) satisfy the conditions of the two tests.
        
        <p>To determine whether the correlation between revenue and the 3 variables (user ratings, gender of cast and budget) is significant, the p-value was compared to the significance level of 0.05. Significance level of 0.05 indicates that there is a 5% chance to conclude that a correlation exists even when it actually does not. The p-value also indicates whether the correlation coefficient of the relationship between two variables is significantly different from 0.
</p>
    
    <p>When the p-value ≤ 0.05, the correlation is said to be “statistically significant” and it can be concluded that the correlation is different from 0.
</p>
    
    <p>When p-value > 0.05, the correlation is said to be “not statistically significant”and it cannot be concluded that the correlation is different from 0.
</p>
    
    
<p>To view a step by step process of how the analysis was executed using Python, click <a href="https://github.com/fivegoldstars/fivegoldstars.github.io/blob/master/python_code/Statistical%20Analysis.ipynb">here.</a></p>
    
    
    <section id="revsratings" class="main"><br><br></section>
    
    <h3><b><u>Revenue vs Ratings</u></b></h3>
    
    <div style=" font-size:80%; text-align:center; max-width: 100%"><img src="http://fivegoldstars.github.io/images/code1.png"  height: 250px  style="padding-bottom:0.5em; max-width: 100%; max-height: 250px"  /></div>
    
    <p>Spearman’s rho correlation coefficient (rs) = 0.296, p-value = 5.87x10-<sup>102</sup>
        <p>Kendall’s tau correlation coefficient (τb) = 0.211 , p-value = 8.31x10-<sup>100</sup>
</p>
    
    <p><em>Spearman’s rho:</em> there was a weak, positive correlation between revenue and ratings, which was statistically significant (p-value ≤ 0.05). Thus, it can be concluded that the correlation is different from 0.
</p>
    
 <p><em>Kendall’s tau:</em> there was a weak, positive correlation between revenue and ratings, which was statistically significant (p-value ≤ 0.05). Thus, it can be concluded that the correlation is different from 0.
       </p>
    
    <section id="revsbudget" class="main"><br><br></section> <h3><b><u>Revenue vs Budget</u></b></h3>
    
    <div style=" font-size:80%; text-align:center; max-width: 100%"><img src="http://fivegoldstars.github.io/images/code2.png"  height: 250px  style="padding-bottom:0.5em; max-width: 100%; max-height: 250px"  /></div>
    
    <p>Spearman’s rho correlation coefficient (rs) = 0.707, p-value = 0.0
        <p>Kendall’s tau correlation coefficient (τb) = 0.523, p-value = 0.0
    
    <p><em>Spearman’s rho:</em>  there was a strong, positive correlation between revenue and budget, which was statistically significant (p-value ≤ 0.05). Thus, it can be concluded that the correlation is different from 0.

</p>
    
 <p><em>Kendall’s tau:</em> there was a fairly strong, positive correlation between revenue and budget, which was statistically significant (p-value ≤ 0.05). Thus, it can be concluded that the correlation is different from 0.
       </p>

    <section id="revsmale" class="main"><br><br></section>
    
    <h3><b><u>Revenue vs Gender (Male Actors)</u></b></h3>
    
    <div style=" font-size:80%; text-align:center; max-width: 100%"><img src="http://fivegoldstars.github.io/images/code3.png"  height: 250px  style="padding-bottom:0.5em; max-width: 100%; max-height: 250px"  /></div>

    <p>Spearman’s rho correlation coefficient (rs) = 0.0215, p-value = 0.591
	<p>Kendall’s tau correlation coefficient (τb) = 0.0152, p-value = 0.597
</p>
    <p><em>Spearman’s rho:</em>  there was a very small, positive correlation between revenue and gender (male actors), which was not statistically significant (p-value > 0.05). Thus, it cannot be concluded that the correlation is different from 0.


</p>
    
 <p><em>Kendall’s tau:</em> there was a very small, positive correlation between revenue and gender (male actors), which was not statistically significant (p-value > 0.05). Thus, it cannot be concluded that the correlation is different from 0.

           </p>
    
    <section id="revsfemale" class="main"><br><br></section>
    
    <h3><b><u>Revenue vs Gender (Female Actors)</u></b></h3>
    
    <div style=" font-size:80%; text-align:center; max-width: 100%"><img src="http://fivegoldstars.github.io/images/code4.png"  height: 250px  style="padding-bottom:0.5em; max-width: 100%; max-height: 250px"  /></div>

    <p>Spearman’s rho correlation coefficient (rs) = - 0.0319, p-value = 0.425

	<p>Kendall’s tau correlation coefficient (τb) = - 0.0222, p-value = 0.441

</p>
    <p><em>Spearman’s rho:</em>  there was a very small, negative correlation between revenue and gender (female actors), which was not statistically significant (p-value > 0.05). Thus, it cannot be concluded that the correlation is different from 0.



</p>
    
 <p><em>Kendall’s tau:</em> there was a very small, negative correlation between revenue and gender (female actors), which was not statistically significant (p-value > 0.05). Thus, it cannot be concluded that the correlation is different from 0.

       </p>
    
         
    <section id="statsconclusion" class="main"><br><br></section> <h4><b><em>Conclusions</em></b></h4>
    
<p>Ratings appeared to be a weak predictor of box office success. Similarly, there was no correlation between gender of cast and revenue. The best indicator of the level of revenue collected by a movie appear to be its budget. The larger the amount of money invested in a film, the more revenue it makes at the box office.



              </p>
                
</section>

                    
								
                
    <section id="results" class="main"><br><br>
    
								<h2><b>Results and Discussion</b></h2>
                                    <h3><b><u>Results</u></b></h3>
        
        
        <p>Our central hypothesis was that film ratings will not predict box office revenue. Our descriptive analysis has demonstrated that films with low ratings received relatively low revenue, and films with moderately high ratings received higher revenue than films with lower ratings.  Yet, it does not show the trend that highly rated films receive the highest revenue. This implies film ratings do not consistently correlate with box office revenue, which supports our central hypothesis. Moreover, this correlation can be further evidenced by our statistical analysis with a weak and positive correlation being found (a rather small Spearman’s rho of 0. 296 and Kendall’s tau of 0.211).

 
</p>
        
        <p>Our second hypothesis states that the gender of actors in films will predict box office success. Our descriptive analysis has indicated that films with moderately high percentage of male actors received higher revenue than those with high percentage of female actors. This connotes film ratings to some extent correlate with box office revenue, despite some exceptions where very high percentage of male actors is not favorable for generating high revenue. The strength of their correlation has been evaluated in the statistical analysis as a very small and positive correlation, with very small Spearman’s rho value of 0.0215 and Kendall’s tau value of 0.0152. Therefore, gender of actors can only predict box office success weakly.</p>
        
        <p>Our third hypothesis predicts that film budget will indicate box office success. Our descriptive analysis has suggested that there is a positive correlation between film revenue and budget, although this is subject to the extreme case where very high budget actually corresponds to a lower revenue. The strength of the correlation has been examined further in the statistical analysis, which revealed  a strong a positive correlation between film budget and box office revenue. Specifically, the Spearman’s r is 0.707 and Kendall’s t is 0.523, which are higher and closer to 1 compared to the ones for the previous 2 hypotheses. Thus, budget of films can predict box office success relatively well, and it is the best predictor of box office success out of the 3 different ones that we have investigated.
        
</p>
        
        
                                <h3><b><u>What Does the Data Tell Us?</u></b></h3>
        
        <p>In theory, highly-rated products/services should sell more as the high rating reflects better quality (Sun, 2011). However, research literature on rating’s influence on purchase intention is mixed. A highly-rated product/service can, but does not always, correlate with making money. Yet whole organisations such as GoodGuide, dedicate themselves to rating products (GoodGuide, 2019). Our research attempted to explore links between ratings and revenue success in an industry well-known for its stars, the film industry. In addition, we investigated two other variables that we considered to affect potential film earnings, film budget and gender of actors in films.</p>
        
        <p>Our statistical analysis found a very weak relationship between ratings and revenue, meaning that film ratings are not necessarily associated with how much money it will make at the box-office. While films with higher ratings did tend to earn more money, the converse was not true. Most films had relatively average ratings to high ratings. However, what is particularly interesting is that this distribution (seen in our bar charts) followed a similar distribution to Hu, Pavlou and Zhang’s j-shaped distribution of product reviews (2009). Their investigation suggests people are more likely to leave a review if their opinions are at either extreme (i.e. those giving 1- and 5-star ratings), as opposed to those in the middle (i.e. 3- and 4- stars). This further supports our hypothesis that ratings are not reliable predictors of film-financial success as the review-system has a skewed bias, which also explains why research results are so varied. Instead it is important to account for rating-prejudice and investigate other factors which may be influencing viewers intentions to watch films.  Moreover, if not a forecast for success, our research calls into question the purpose of ratings in the first place. As the industry competes with the likes of subscription film services like Netflix, this investigation encourages film producers to uncover what truly drives viewers to the cinema.</p>
        
        <p>The relationship between the gender of actors/ film budget and ratings as predictors of revenue success found contrasting results. Although men appeared to bring a higher revenue than women, there was no statistically significant relationship between having male or female actors in films and how much money films made. This result was inconclusive. This means there may be another factor interacting with actors’ genders which affects film-revenue e.g. how famous an actor is. Other studies have pointed at the impact of celebrities on film-success (Treme & Craig, 2013) which could explain why famous models or singers are often cast. In contrast, there was a very strong relationship between a film’s budget and its revenue, suggesting the importance of having money to begin with. Interestingly, films with a very high budget did not make as much money, recommending that budget is not the only indicator of success. Nevertheless, out of the variables we measured, budget acted as the strongest predictor.

 </p>
        
        <p>Overall, interesting conclusions can be drawn from this project to influence film-producers’ strategies and question the role of ratings in general. Firstly, as actors’ genders have no/ questionable influence on film success there should be a more active role to address the gender imbalance in films (Women and Hollywood, 2018) given it will not impact film profit. Secondly, as budget does contribute to film-revenue, the industry should be aware of the struggles small film-production companies/ independent films face in succeeding and fund smaller teams to encourage diversity and avoid a monopoly by larger production companies. Finally, the value of ratings should be viewed critically across both film and other industries. Looking at e.g. the social value of ratings (Cheung et al., 2003) might be more useful. That is, ratings act as a social influencer by swaying recommendations e.g. a friend might recommend a film because of how terrible it is. Mining ratings by matching customer interests may therefore be a more effective way of influencing purchasing decision than assuming higher ratings automatically translate into more purchases and greater profit.</p>
                                    
                </section>
                
                 <section id="limits" class="main">
                     <h2><b>Limitations</b></h2>
                     
                     <p>Before accepting our findings, we should acknowledge certain limitations to our research.</p>
                     
                     <ol>
                         <li>We did not consider the difference that inflation has on our data concerning budget and revenue. Although initially we wanted to only look at films between 2015-2018 to limit the effects of inflation, when cleaning the data we realised this would not leave us with enough films to carry out statistical analysis.</li>
                         <li>Age restrictions on films limits their audience. This in turn may have impacted a film’s box-office success as films which can be viewed by a limited audience might sell less tickets. Nevertheless, films open to all will be those rated U and PG which tend to be children’s films. These might attract a limited audience because of the genre and therefore limit film-revenue although not to a great extent.

 </li>
                         <li>The distribution of our data meant we had to use non-parametric tests which are less accurate than parametric tests. However, as our data did not meet the prerequisites of parametric tests, it was inevitable that we would not be able to do this type of statistical analysis.</li>
                         <li>We cannot assume that actors’ genders and film-budget are the only factors affecting film-revenue success. For instance, a film’s marketing could affect how many viewers it gets; release date could increase/decrease competition i.e. if many films are released at the same time, they might struggle more to get viewers; foreign films might struggle against American/English films because of language barriers and actor popularity might entice audiences regardless of the quality of the film.

 </li>
                         <li>Unpredictability of consumer behaviour may be a limiting factor in comparing ratings and film financial success e.g. viewers may choose to see a film because of how bad it is.

</li>
                         <li>Data for 62 films went missing during our statistical and visual analysis which might have affected our results.

</li>
                         <li>What is considered as a high or low rating was defined by our judgement which may be subjective in comparison to the general population.</li>
                         <li>Finally, although we encountered a strong relationship between film budget and film revenue, we cannot assume that correlation is equal to causation. Nevertheless, budget should still be a matter of concern to the film industry in providing funding and access for smaller film productions to take place.

 

 </li>
                     
                     </ol>
                     
                     <p>Despite these limitations, there are still valuable conclusions to draw from our project. Namely, a call to re-evaluate the role ratings play in consumer purchase intention and how this can be harnessed. Furthermore, the data analyses carried out serve to mitigate limitations. Larger-scale research into the effect of ratings in other industries e.g. travel, would serve to uncover their value and what influence they truly exert.</p>
                </section>
                
                 <section id="ref" class="main">
                     <h2><b>References</b></h2>
                     
                     <h3><b><u>Why This Project? </u></b></h3>
                     
                     <ul>
                         
                         <li>Brueggemann, T. (2017). Marvel vs. DC at the Box Office: One Comes out on Top — but Not by Much. [online] IndieWire. Available at: https://www.indiewire.com/2017/11/marvel-vs-dc-box-office-rivalry-thor-ragnarok-justice-league-1201897782/ [Accessed 21 Jan. 2019].
</li>
                         
                         <li>Buckley, C. (2018). From Black Panther to Crazy Rich Asians: How 2018 finally proved to studios that actors of colour have global appeal. [online] The Independent. Available at: https://www.independent.co.uk/arts-entertainment/films/features/black-panther-crazy-rich-asians-blackkklansmen-2018-film-actors-of-colour-global-appeal-a8700591.html [Accessed 21 Jan. 2019]. 
</li>
                     <li> 
Connick, T. (2018). Black Mirror's 'Nosedive' episode is about to become reality in China - NME. [online] NME. Available at: https://www.nme.com/news/tv/black-mirrors-nosedive-episode-become-reality-china-2263309 [Accessed 21 Jan. 2019].
</li>
                         
                         <li>Gray, T.  (2019). Jon M. Chu on ‘Crazy Rich Asians’: ‘We Had a Sense of Purpose’. [online] Variety. Available at: https://variety.com/2019/film/news/jon-chu-crazy-rich-asians-1203105999/ [Accessed 21 Jan. 2019].
</li>
                     <li>Hall, C. and Boyle, B. (2019). Google Lens: What is it and how does it work? - Pocket-lint. [online] Pocket-lint. 
Available at: https://www.pocket-lint.com/apps/news/google/141075-what-is-google-lens-and-how-does-it-work-and-which-devices-have-it 
[Accessed 21 Jan. 2019]. 
</li>
                     
                     <li>McCarthy, K. (2016). Yelp-for-people app Peeple is back – so we rated Julia, its cofounder. [online] Theregister.co.uk. Available at: https://www.theregister.co.uk/2016/03/08/peeple_app_back_from_grave/ [Accessed 21 Jan. 2019].
</li></ul>
                     
                     
                     
                     <h3><b><u>Literature Review </u></b></h3>
                     
                     <ul>
                     
                     <li>Austin, B. (1983). Critics’ and Consumers’ Evaluations of Motion Pictures. Journal of Popular Film and Television, 10(4), pp.156-167.</li>
                         
                    <li>Basuroy, S., Chatterjee, S. and Ravid, S. (2003). How Critical are Critical Reviews? The Box Office Effects of Film Critics, Star Power, and Budgets. Journal of Marketing, 67(4), pp.103-117.</li>
                     
                    <li>Berg, J. and Raddick, M. (2016). First You Get the Money, Then You Get the Reviews, Then You Get the Internet Comments: A Quantitative Examination of the Relationship Between Critics, Viewers, and Box Office Success. Quarterly Review of Film and Video, 34(2), pp.101-129.</li>
                         
                     <li>Brewer, S., Kelley, J. and Jozefowicz, J. (2009). A blueprint for success in the US film industry. Applied Economics, 41(5), pp.589-606.</li>
                         
                         <li>Collins, A., Hand, C. and Snell, M. (2002). What makes a blockbuster? Economic analysis of film success in the United Kingdom. Managerial and Decision Economics, 23(6), pp.343-354.</li>
                         
                    <li>Desai, K. and Basuroy, S. (2005). Interactive influence of genre familiarity, star power, and critics' reviews in the cultural goods industry: The case of motion pictures. Psychology and Marketing, 22(3), pp.203-223.</li>
                         
                         <li>Eliashberg, J. and Shugan, S. (1997). Film Critics: Influencers or Predictors?. Journal of Marketing, 61(2), p.68.</li>
                         
                         <li>Gaenssle, S.,  Budzinski, O. and Astakhova, D. (2018).  Conquering the Box Office: Factors Influencing Success of International Movies in Russia. Ilmenau Economics Discussion Papers, 24 (113). Available at SSRN: https://ssrn.com/abstract=3193629</li>
                         
                         <li>Google. (2013). Quantifying Movie Magic with Google Search. Available at: https://ssl.gstatic.com/think/docs/quantifying-movie-magic_research-studies.pdf 
</li>
                         
                         <li>Help.imdb.com. (n.d.). IMDb | Help. [online] Available at: https://help.imdb.com/article/imdb/track-movies-tv/faq-for-imdb-ratings/G67Y87TFYYP6TWAV# [Accessed 22 Jan. 2019].
</li>
                         
                         <li>King, T. (2007). Does film criticism affect box office earnings? Evidence from movies released in the U.S. in 2003. Journal of Cultural Economics, 31(3), pp.171-186.</li>
                         
                         <li>Kusumasondjaja, S., Shanka, T. and Marchegiani, C. (2012). Credibility of online reviews and initial trust. Journal of Vacation Marketing, 18(3), pp.185-195.</li>
                         
                         <li>Maslowska, E., Malthouse, E. and Bernritter, S. (2016). The Effect of Online Customer Reviews’ Characteristics on Sales. Advances in Advertising Research. 7.  pp.87-100.</li>
                         
                         <li>Ravid, S. (1999). Information, Blockbusters, and Stars: A Study of the Film Industry. The Journal of Business, 72(4), pp.463-492.</li>
                         
                         <li>Smith, S. and Smith, V. (1986). Successful movies: A preliminary empirical analysis. Applied Economics, 18(5), pp.501-507.</li>
                     
                     </ul>
                     
                     <h3><b><u>Methodology</u></b></h3>
                     
                     <p><ul>
                     
                     <li>Field, A. 2005. Discovering Statistics using SPSS.</li>
                     
                     <li>Laerd Statistics. 2018. Spearman’s Rank Order Correlation. [online]. Available from: https://statistics.laerd.com/statistical-guides/spearmans-rank-order-correlation-statistical-guide.php [Accessed 12 Jan 2019].</li>
                     
                     <li>Statistics Solutions. 2018. Kendall’s Tau and Spearman’s Rank Correlation Coefficient. [online]. Available from: https://www.statisticssolutions.com/kendalls-tau-and-spearmans-rank-correlation-coefficient/ [Accessed 12 Jan 2019].</li>
                     </ul>
                     
                     <h3><b><u>Polar Chart</u></b></h3>
                     
                     <ul>
                     
                     <li>Brewer, S., Kelley, J. and Jozefowicz, J. (2009). A blueprint for success in the US film industry. Applied Economics, 41(5), pp.589-606</li></ul>
                     
                     <h3><b><u>Descriptive Analysis</u></b></h3>
                     
                     <ul>
                     
                     
                     <li>Mercer.D, Stackoverflow. 2017. How to save a seaborn plot into a file. Available from: https://stackoverflow.com/questions/32244753/how-to-save-a-seaborn-plot-into-a-file [Accessed 8 Jan 2019].</li>
                     <li>Sjobeek, Github. 2014. Simple way to set figsize for any plot. [online]. Available from: https://github.com/mwaskom/seaborn/issues/112 [Accessed 12 Jan 2019].</li>
                     <li>Waskom,M. 2018. Seaborn.regplot. [online]. Available from: https://seaborn.pydata.org/generated/seaborn.regplot.html [Accessed 28 Dec 2018].</li>
                     <li>Waskom,M. 2018. Controlling figure aesthetics. [online]. Available from: https://seaborn.pydata.org/tutorial/aesthetics.html [Accessed 30 Dec 2018].</li>
                     <li>Waksom,M. 2018. Seaborn.violinplot. [online]. Available from: https://seaborn.pydata.org/generated/seaborn.violinplot.html [Accessed 30 Dec 2018].</li>
                     <li>Waksom,M. 2018. Seaborn.barplot. [online]. Available from: https://seaborn.pydata.org/generated/seaborn.barplot.html [Accessed 30 Dec 2018].</li>
                     <li>Waksom,M. 2018. Seaborn.lmplot. [online]. Available from: https://seaborn.pydata.org/generated/seaborn.lmplot.html [Accessed 3 Jan 2019].</li>
                     <li>Waksom,M. 2018. Visualizing the distribution of a dataset. [online]. Available from: https://seaborn.pydata.org/tutorial/distributions.html [Accessed 15 Jan 2019].</li></ul>
                     
                     <h3><b><u>Statistical Analysis</u></b></h3>
                     <ul>
                     
                     <li>Kite, your programming copilot. 2018. How to: scipy. [online]. Available from: https://kite.com/python/examples/665/scipy-compute-the-spearman-correlation [Accessed 5 Jan 2019].</li>
                         <li>Kite, your programming copilot. How to:scipy. [online]. Available from: https://kite.com/python/examples/729/scipy-compute-kendall's-tau-coefficient-between-2-arrays [Accessed 5 Jan 2019].</li>
                         
                         </ul>
                     
                     <h3><b><u>What the Data Tells Us</u></b></h3>
                     
                     <ul>
                     
                     <li> Cheung, K et al. 2003. Mining customer product ratings for personalized marketing. Decision Support Systems [online], 35, pp. 231-243. Available from: https://ac.els-cdn.com/S0167923602001082/1-s2.0-S0167923602001082-main.pdf?_tid=7edd3f12-ecce-4aed-a0d4-4b0de1a98213&acdnat=1547571348_104041205dda2300b4b2ef5dc63d1d1c [Accessed 14 Jan 2019].</li>
                     <li>GoodGuide. 2019. Let us guide you to what’s good. [online]. Available from: https://www.goodguide.com/#/ [Accessed 14 Jan 2019].</li>
                     <li>Hu, N; Pavlou, P & Zhang, J. 2009. Overcoming the J-shaped Distribution of Product Reviews. Technical Opinion. [online], 52(10), pp.144-147. Availble from: http://delivery.acm.org/10.1145/1570000/1562800/p144-hu.pdf?ip=144.82.8.29&id=1562800&acc=ACTIVE%20SERVICE&key=BF07A2EE685417C5%2ED93309013A15C57B%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1547571137_7ee587b02e94223be958320161059cc0 [Accessed 14 Jan 2019].</li>
                     <li>Sun, M. 2011. How Does the Variance of Product Ratings Matter? Management Science [online], 58(4), pp.iv-842. Available from: https://pubsonline.informs.org/doi/abs/10.1287/mnsc.1110.1458 [Accessed 14 Jan 2019].</li>
                     <li>Treme, J & Craig, L. 2013. Celebrity star power: Do age and gender effects influence box office performance?. Applied Economics Letters, [online], 20(5), pp.440-445. Available from: https://www.tandfonline.com/doi/pdf/10.1080/13504851.2012.709594?needAccess=true [Accessed 14 Jan 2019].</li>
                     <li>Treme, J & Craig, L. 2013. Celebrity star power: Do age and gender effects influence box office performance?. Applied Economics Letters, [online], 20(5), pp.440-445. Available from: https://www.tandfonline.com/doi/pdf/10.1080/13504851.2012.709594?needAccess=true [Accessed 14 Jan 2019].</li>
                         <li>Women and Hollywood. 2018. 2017 Statistics: Women Onscreen. [online]. Available from: https://womenandhollywood.com/resources/statistics/2017-statistics/ [Accessed 14 Jan 2019].</li>
                     </ul>
                   
                </section>
                        

					</div>

				<!-- Footer -->
					<footer id="footer">
							<h2 align="center">Group 11:</h2>
							<p> Carlota Veal Baschwitz, Christina Jie Ying Wang, Sara Hüttl, Szymon Hawryszko, Tara Nelson  </p>
						<p class="copyright">Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
					</footer>

			</div>
		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
